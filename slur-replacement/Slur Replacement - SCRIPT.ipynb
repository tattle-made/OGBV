{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from fuzzywuzzy import process\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import Levenshtein as lev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save slurs in a file\n",
    "\n",
    "slurs_list_lower = ['#rheachakraborty',\n",
    " '#498a',\n",
    " '#feministmafia',\n",
    " '#carryminatiroast',\n",
    " '#justiceforswapnilpandey',\n",
    " '#arrestranaayyub',\n",
    " '#boycottfabindia',\n",
    " '#domesticviolence',\n",
    " '#dowry',\n",
    " '#falsecases',\n",
    " '#feministsaretrash',\n",
    " '#genderbiasedlaws',\n",
    " '#hiteshachandranee',\n",
    " '#love_nahi_jeehad_hai',\n",
    " '#justiceforcabdriver',\n",
    " '#menscommission',\n",
    " '#menslivesmatter',\n",
    " '#potentiallegalterrorist',\n",
    " '#mentoo',\n",
    " '#womenaretrash',\n",
    " 'presstitutes',\n",
    " 'liberal',\n",
    " '#роЕродрпНродрпИ',\n",
    " '#speakupmen',\n",
    " '#ranaayyub',\n",
    " '#sullideals',\n",
    " '#pussypower',\n",
    " '#reservation_kills_merit',\n",
    " '#reservation',\n",
    " '#superstraight',\n",
    " '#randikutiya',\n",
    " '#save_hindu_girls',\n",
    " 'рдЬрд┐рд╣рд╛рджреА',\n",
    " 'pseudohindus',\n",
    " 'chrislamocommies',\n",
    " 'sanghans',\n",
    " 'nationalists',\n",
    " 'whore',\n",
    " 'faggot',\n",
    " 'love jihad',\n",
    " 'рд░рдВрдбреА',\n",
    " 'рд░рдгреНрдбреА',\n",
    " 'libtard',\n",
    " 'рд░рдВрдбреАрдЦрд╛рдирд╛',\n",
    " 'deshdrohi chinese',\n",
    " 'рд░рдгреНрдбреА рд░реЛрдирд╛',\n",
    " 'рд▓реБрд▓реНрд▓реА',\n",
    " 'рдЧрд╛рдВрдб',\n",
    " 'рдХреБрддрд┐рдпрд╛',\n",
    " 'рдХреБрддреНрддреА',\n",
    " 'рдХрдореАрдиреА',\n",
    " 'роЕродрпНродрпИ',\n",
    " 'роЯро┐роХро┐',\n",
    " 'роЕро░ро╡ро╛рогро┐',\n",
    " 'роЕро▓ро┐',\n",
    " 'роХро▓рпНро▓рпБ',\n",
    " 'рокрпКройрпНро╕рпН',\n",
    " 'роТройрпНрокродрпБ',\n",
    " 'рдореБрдЬрд░рд╛',\n",
    " 'рд╣рд┐рдЬреЬрд╛',\n",
    " 'рдмрджрд╕реВрд░рдд',\n",
    " 'рдмрд▓рд╛рддреНрдХрд╛рд░',\n",
    " 'рд▓рд┐рдмреНрд░рд╛рдгреНрдбреВ',\n",
    " 'рдирдВрдЧреА_рдкреБрдВрдЧреА',\n",
    " 'рдкрд╛рдЧрд▓ рдФрд░рдд',\n",
    " 'rape',\n",
    " 'r@pe',\n",
    " 'r@p3',\n",
    " 'bitch',\n",
    " 'victim_card',\n",
    " 'sekoolar',\n",
    " 'sickular',\n",
    " 'sc0undrel',\n",
    " 'r@ndi',\n",
    " 'feminazi',\n",
    " 'chinese corona',\n",
    " 'рокрпКроорпНрокро│ рокрпКро░рпБроХрпНроХро┐',\n",
    " 'роЩрпНроХрпЛродрпНродро╛',\n",
    " 'роХрпЛродрпНродро╛',\n",
    " 'родро╛ропрпЛро│ро┐',\n",
    " 'родрпЗро╡рпНроЯро┐ропро╛ рокрпИропро╛',\n",
    " 'родрпЗро╡рпНроЯро┐ропро╛',\n",
    " 'роЪрпВродрпНродрпБ',\n",
    " 'ро▓рпЖро╕рпНрокро┐ропройрпН',\n",
    " 'роКроорпНрокрпБ',\n",
    " 'рокрпБрогрпНроЯ',\n",
    " 'роРроЯрпНроЯроорпН',\n",
    " 'роЕропро┐роЯрпНроЯроорпН',\n",
    " 'роЪро╛рооро╛ройрпН',\n",
    " 'роХрпВродро┐',\n",
    " 'роЖроЯрпНроЯроХрпНроХро╛ро░ро┐',\n",
    " 'ро╡рпЗроЪрпИ',\n",
    " 'ро╡рпЗроЪ',\n",
    " 'рокрпКродрпБроЪрпН роЪрпКродрпНродрпБ',\n",
    " 'роКро░рпН роорпЗропрпНро▒родрпБ',\n",
    " 'рокрпКроЯрпНроЯрпИ',\n",
    " 'рооро╛роЩрпНроХро╛',\n",
    " 'роХрпЛро┤ро┐',\n",
    " 'роорпБро▓рпИ',\n",
    " 'рокро▒родрпНродро╛ропрпЛро▓ро┐',\n",
    " 'роУроХрпНроХ',\n",
    " 'родрпЗро╡роЯро┐ропро╛_рооро╡ройрпН',\n",
    " 'родрпЗро╡роЯро┐ропро╛ рокроЪроЩрпНроХро│ро╛',\n",
    " 'рокрпБрогрпНроЯрпИ',\n",
    " 'atrocity_act',\n",
    " 'arrestlucknowgirl',\n",
    " 'fakecasewalibiwi',\n",
    " 'gasti',\n",
    " '#lovejihad',\n",
    " '#misandry',\n",
    " '#piddilover',\n",
    " '#pseudohindus',\n",
    " '#rheality_today',\n",
    " '#sanghans',\n",
    " '#saveoursons',\n",
    " '#scstact',\n",
    " 'ЁЯНС',\n",
    " '#рдЬрд┐рд╣рд╛рджреА_рдореБрдХреНрдд_рдирд╡рд░рд╛рддреНрд░рд┐',\n",
    " '#роУродрпНрод',\n",
    " 'рдЖрддрдВрдХрд╡рд╛рджреА',\n",
    " 'рдХрдЪрд░рд╛',\n",
    " 'рдХрдВрдЬрд░',\n",
    " 'рдХрдЯрд╛ рд▓реБрдВрдб',\n",
    " 'рдХрд▓рдореБрдВрд╣реА',\n",
    " 'рдХрд╕рд╛рдИ',\n",
    " 'рдХрд╛рдо_рд╡рд╛рд▓реА_рдмрд╛рдИ',\n",
    " 'рдХрд╛рд▓рд╛ рдЯреЗрдВрдЯ',\n",
    " 'рдХрд╛рд▓реА рдХрд▓реВрдЯреА',\n",
    " 'рдХрд╛рд▓реЗ рддрд╡реЗ',\n",
    " 'рдХрд┐рдиреНрдирд░',\n",
    " 'рдХреБрд▓реНрдЯрд╛',\n",
    " 'реЩреБрд╕рд░рд╛',\n",
    " 'рдЧрдЯрд░ рдкреИрджрд╛рдЗрд╢',\n",
    " 'рдЧрдгрд┐рдХрд╛',\n",
    " 'рдЧрджреНрджрд╛рд░',\n",
    " 'рдЧрдзреА',\n",
    " 'рдШрдЯрд┐рдпрд╛ рдФрд░рдд',\n",
    " 'рдЪрдВрдбрд╛рд▓',\n",
    " 'рдЪрдВрдбрд╛рд▓_рдЪреМрдХрдбрд╝реА',\n",
    " 'рдЪрдордЪрд╛',\n",
    " 'рдЪрдорд╛рд░',\n",
    " 'рдЪрд░рд┐рддреНрд░рд╣реАрди',\n",
    " 'рдЪрд╛рдЯреБрдХрд╛рд░',\n",
    " 'рдЪрд╛рд▓реВ рдФрд░рдд',\n",
    " 'рдЪреБрджрд╛рдИ',\n",
    " 'рдЪреБрд╕рд▓реНрдорд╛рди',\n",
    " 'рдЪреБрд╕реНрд▓рд╛рдорд┐',\n",
    " 'рдЪреБрд╕реНрд▓рд┐рдо',\n",
    " 'рдЪреВрдЪреА',\n",
    " 'рдЪреВреЬрд╛',\n",
    " 'рдЪреВрд╕',\n",
    " 'рдЫрд┐рдирд╛рд▓',\n",
    " 'рдЬрдВрдЧрд▓реА',\n",
    " 'рдЬрд▓реА',\n",
    " 'рдЬрд╛рд╣рд┐рд▓_рдФрд░рдд',\n",
    " 'рдЬреВрддрд╛ рдЦрд╛рдпреЗрдЧреА',\n",
    " 'рдЬреЛрд░реВ рдХрд╛ рдЧреБрд▓рд╛рдо',\n",
    " 'рдЭреВрдареА рдФрд░рдд',\n",
    " 'рдЫреБрддреАрдпреЗ',\n",
    " 'рддрд╡рд╛рдЗреЮ',\n",
    " 'рджрд▓рд╛рд▓',\n",
    " 'рджреЗрд╣рд╛рддрди',\n",
    " 'рджреЗрд╣рд╛рддреА рдФрд░рдд',\n",
    " 'рджреЛ рдХреМреЬреА рдХреА рдФрд░рдд',\n",
    " 'рджреЛрдЧрд▓реА',\n",
    " 'рдзреЛрдмреА',\n",
    " 'рдирд▓реНрд▓реА',\n",
    " 'рдирд╛рдЬрд╛рдпреЫ',\n",
    " 'рдирд╛рд▓',\n",
    " 'рдкрдиреМрддреА',\n",
    " 'рдкрд┐рдЫрд╡рд╛реЬрд╛',\n",
    " 'рдкреЗрд▓',\n",
    " 'рдкреИрд░ рдХреА рдЬреВрддреА',\n",
    " 'рдкреЙрдЯреА',\n",
    " 'рдлреЗрдорд┐рдирд┐рдЬрдо',\n",
    " 'рдмрддреНрддрдореАреЫ',\n",
    " 'рдмрд╣рди рдХреА рд▓реЛреЬреА',\n",
    " 'рдмрд╣рди рдЪреЛрдж',\n",
    " 'рдмрд╣рдирдЬреА',\n",
    " 'рдмрд╛реЫрд╛рд░реВ рдФрд░рдд',\n",
    " 'рдмреАрдмреА',\n",
    " 'рдмреБрд░рдЦрд╛ рдзрддреНрдд',\n",
    " 'рдмреБрд░рдЦреЗ рд╡рд╛рд▓реА',\n",
    " 'рдмреБрд▓реНрд▓реА',\n",
    " 'рдмреЗрд╣реВрджрд╛ рдФрд░рдд',\n",
    " 'рдмреИрд▓ рдмреБрджреНрдзрд┐',\n",
    " 'рднрдВрдЧреА',\n",
    " 'рднреЬрд╡рд╛',\n",
    " 'рднрджреНрджреА рдФрд░рдд',\n",
    " 'рднрд╛рдВрдб',\n",
    " 'рднрд╛рдВрдб рдФрд░рдд',\n",
    " 'рднрд╛реЬреЗ рдХрд╛ рдЯрдЯреНрдЯреВ',\n",
    " 'рднрд╛рд░рдд рддреЗрд░реЗ рдЯреБрдХрдбрд╝реЗ рдЧреИрдВрдЧ',\n",
    " 'рднреВрддрдиреА рдХреЗ',\n",
    " 'рднреЛрдВрдХрдирд╛',\n",
    " 'рднреЛрд╕рдбрд╝реАwala',\n",
    " 'рдорд░ рдХреНрдпреЛрдВ рдирд╣реАрдВ рдЬрд╛рддреА',\n",
    " 'рдорд╣рд╛рд░',\n",
    " 'рдорд╛ рдХреЗ рднреЛрд╕рдбрд╝реЗ',\n",
    " 'рдорд╛рджрд░рдЬрд╛рдд',\n",
    " 'рдореБрд▓рд┐рдпрд╛',\n",
    " 'рдореБрдБрд╣ рдореИрдВ рд▓реЗ',\n",
    " 'рдореВрдд',\n",
    " 'рдореЗрдбрдо рдЬреА',\n",
    " 'рдореЛрдЯреА рднреИрдВрд╕',\n",
    " 'рдореНрд▓реЗрдЪреНрдЫрд╛',\n",
    " 'рд░рд╛рдВрдб',\n",
    " 'рд▓рд╛рдирддреА',\n",
    " 'рд▓реЗрд╕реНрдмрд┐рдпрди',\n",
    " 'рд▓реЛреЬреВ',\n",
    " 'рд▓реМреЬрд╛',\n",
    " 'рд▓реМреЬреЗ',\n",
    " 'рд╡рд┐рдХреНрдЯрд┐рдо рдХрд╛рд░реНрдб',\n",
    " 'рд╢рд░рд┐рдпрд╛',\n",
    " 'рд╕рд╛рд▓реА',\n",
    " 'рд╕реБрд▓реНрд▓реА',\n",
    " 'рд╕реВрдЕрд░',\n",
    " 'рд╕реВрдЕрд░ рдХреА рдФрд▓рд╛рдж',\n",
    " 'рд╕реМ рдЯрдХрд╛ рдЯрдВрдЪ рдорд╛рд▓',\n",
    " 'рд╕реНрдХрд░реНрдЯ рд╡рд╛рд▓реА рдмрд╛рдИ',\n",
    " 'рд╣рдЧрдирд╛',\n",
    " 'рд╣рдмрд╢реА',\n",
    " 'рд╣рд░рд╛рдордЬрд╛рджреА',\n",
    " 'рд╣рд░рд╛рдореА',\n",
    " 'рд╣рд▓рд╛рд▓рд╛',\n",
    " 'рд╣рд┐рдЬрд░рд╛',\n",
    " 'рд╣рд┐рдЬрд╝рд░рд╛рдкрдВрддреА',\n",
    " 'рд╣рд┐рд▓рд╛рдУрдЧреА',\n",
    " 'рдорд╛рджрд░рдЪреЛрдж',\n",
    " 'рднреЛрд╕реНрдбреАрдХреЗ',\n",
    " 'рдЯрдЯреНрдЯреА',\n",
    " 'рдЬрд╛ рдирд╛рд▓реА рд╕рд╛рдлрд╝ рдХрд░рдХреЗ рдЖ',\n",
    " 'рдЖрдВрдЯреА',\n",
    " 'ро▓рпВроЪрпБ роХрпВ',\n",
    " 'рокрпКроЯрпНроЯрпИ роиро╛ропрпН',\n",
    " 'ро▓рпВроЪрпБ',\n",
    " 'родрпЗро╡ро┐роЯро┐ропро╛ро│рпБроХрпНроХрпБ рокрпКро▒роирпНродро╡ройрпН',\n",
    " 'родрпЗро╡ро┐роЯро┐ропро╛ рокрпБрогрпНроЯ',\n",
    " 'роЪрпВродрпНродроЯро┐',\n",
    " 'роХрпВfire',\n",
    " '#рдЫрдХреНрдХрд╛',\n",
    " '#роХрпКроорпНрооро╛',\n",
    " '#роЩрпНроХрпЛродрпНродро╛',\n",
    " '#роЪро╛рооро╛ройрпН',\n",
    " '#рокрпЖро░ро┐ропро╛ро░ро╛ро╡родрпБ_рооропро┐ро░ро╛ро╡родрпБ',\n",
    " '#рокрпКроЯрпНроЯрпИ',\n",
    " '#роорпБроЯрпНроЯро╛роЙрокро┐',\n",
    " 'anti national tukde',\n",
    " '498a',\n",
    " 'bimbo',\n",
    " 'toxic aunty',\n",
    " 'toilet saaf',\n",
    " 'buzlim',\n",
    " 'characterless woman',\n",
    " 'chinky',\n",
    " 'black pepper',\n",
    " 'dull-it',\n",
    " 'dynast sycophants',\n",
    " 'ferrorists',\n",
    " 'izzlam',\n",
    " 'katwa',\n",
    " 'muzlim',\n",
    " 'naachne waali',\n",
    " 'ola u uber',\n",
    " 'pak agents',\n",
    " 'pheminist',\n",
    " 'pissfull',\n",
    " 'rice bags',\n",
    " 'scumbags',\n",
    " 'secular',\n",
    " 'sissy',\n",
    " 'dented-painted',\n",
    " 'toilet + saaf',\n",
    " 'sunflowerрогрпНроЯрпИ_emoji',\n",
    " 'sunflowerрогрпНроЯрпИ',\n",
    " 'scumreds',\n",
    " 'samlaingik',\n",
    " 'ma ki chui',\n",
    " 'jersey cow',\n",
    " 'burnol',\n",
    " 'victim card']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_exact_slurs(tweet,slurs_list_lower):\n",
    "    \n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    matches = re.findall(r\"(?=(\"+'|'.join(slurs_list_lower)+r\"))\", tweet.lower())\n",
    "    \n",
    "    slurs = []\n",
    "    tokens = []\n",
    "    print(matches)\n",
    "    \n",
    "    for match in matches:\n",
    "    \n",
    "        print(match)\n",
    "        \n",
    "        slurs.append(match)\n",
    "        \n",
    "        tokens.append(match)\n",
    "        \n",
    "        tweet = tweet.replace(match,'----')\n",
    "    \n",
    "        # token,slur key pair\n",
    "        result.update({match:match})\n",
    "    \n",
    "    return tweet,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_matching_slurs(tweet,slurs_list_lower,threshold_score=70):\n",
    "    \n",
    "    print(\"\\nApprox matching\")\n",
    "    check = 0\n",
    "         \n",
    "    match_dict = dict(process.extract(tweet,slurs_list_lower,limit = 10,scorer=fuzz.partial_ratio))\n",
    "\n",
    "    matches = match_dict.keys()\n",
    "    \n",
    "    # to compare the distance of top 10 matching slurs to find the right matching\n",
    "    dis_dict = {}\n",
    "    token_slur_dict = {}\n",
    "    \n",
    "    \n",
    "    for slur in matches:\n",
    "        \n",
    "        for token in tweet.split(' '):\n",
    "            \n",
    "            \"\"\"\n",
    "            Can add memoization here\n",
    "            \n",
    "            -Check if the distance b/w token and match is already calculated\n",
    "            \"\"\"\n",
    "            \n",
    "            if (token,slur) not in token_slur_dict:\n",
    "                \n",
    "                dis = lev.distance(token,slur)\n",
    "                            \n",
    "                token_slur_dict[(token,slur)] = dis\n",
    "            \n",
    "                if dis in dis_dict:\n",
    "                    dis_dict[dis].append((token,slur))\n",
    "            \n",
    "                else:\n",
    "                    dis_dict[dis] = [(token,slur)]\n",
    "            \n",
    "    \n",
    "    dist_sort = dict(sorted(dis_dict.items()))\n",
    "   \n",
    "    result_dict = {}\n",
    "    result = {}\n",
    "    \n",
    "    for dist,match in dist_sort.items():\n",
    "        \n",
    "        #print(dist,match)\n",
    "        loop_break = 0\n",
    "        \n",
    "        for token,slur in match:\n",
    "            \n",
    "            #does it work for hin and tamil?\n",
    "            if token:\n",
    "                \n",
    "                print(slur)\n",
    "                print(match_dict[slur])\n",
    "    \n",
    "                if (slur[0].lower() == token[0].lower()) and (match_dict[slur] >= threshold_score) and (token.lower() not in ['muslim','muslims']):\n",
    "                    \n",
    "                    print(f'slur,token : {slur} {token}')\n",
    "                    result_dict[('slur','token')] = (slur,token)\n",
    "                    \n",
    "                    #token,slur value pair\n",
    "                    result.update({token:slur})\n",
    "                    \n",
    "                  \n",
    "                    tweet = tweet.replace(token,'----')\n",
    "                    \n",
    "                    # to iterate all the matches in that dict\n",
    "                    loop_break = 1\n",
    "                    \n",
    "        if loop_break:\n",
    "            break\n",
    "                \n",
    "    return tweet,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slur_replacement_slurs_v1(tweet,slurs_list_lower,threshold_score=70):\n",
    "    \n",
    "    tweet1,exact_result = regex_exact_slurs(tweet,slurs_list_lower)\n",
    "    print(f'exact : {exact_result}')\n",
    "    tweet2,approx_result = approx_matching_slurs(tweet1,slurs_list_lower,threshold_score=threshold_score)\n",
    "    print(f'approx : {approx_result}')\n",
    "    \n",
    "    exact_result.update(approx_result)\n",
    "    print(f'combined : {exact_result}')\n",
    "    \n",
    "    return exact_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweet = 'рдмреНрд░рд╛рд╣реНрдордгрд╡рд╛рджреА рдкрд┐рддреГрд╕рддреНрддрд╛ рдХреНрдпрд╛ рд╣реИ? рдпреЗ рджреЛ рд╢рдмреНрдж рд╣реИ рд▓реЗрдХрд┐рди рдПрдХ рджреВрд╕рд░реЗ рд╕реЗ рдЬреБрдбрд╝реЗ рд╣реБрдП рд╣реИ. рдмреНрд░рд╛рд╣реНрдордгрд╡рд╛рдж рдорд╛рдиреЗ рдЬреЛ рд╕рдорд╛рдЬ рдореЗрдВ рдЬрд╛рддрд┐рд╡рд╛рдж рдХреЛ рдмрдирд╛рдП рд░рдЦрдирд╛ рдЪрд╛рд╣рддрд╛ рд╣реИ рдФрд░ рдкрд┐рддреГрд╕рддреНрддрд╛, рдорд╛рдиреЗ рдЬреЛ рдорд╣рд┐рд▓рд╛рдУрдВ рдкрд░ рдЕрдкрдирд╛ рдЕрдзрд┐рдХрд╛рд░ рдЬрдорд╛рдП рд░рдЦрдирд╛ рдЪрд╛рд╣рддрд╛ рд╣реИ. рдмреНрд░рд╛рд╣реНрдордгрд╡рд╛рджреА рдкрд┐рддреГрд╕рддреНрддрд╛ рдЬреЛ рдЬрд╛рддрд┐ рд╡реНрдпрд╡рд╕реНрдерд╛ рдФрд░ рдкрд┐рддреГрд╕рддреНрддрд╛ рджреЛрдиреЛрдВ рдХреЛ рдмрдирд╛рдпреЗ рд░рдЦрддреА рд╣реИ.'\n",
    "slur_replacement_slurs_v1(tweet,slurs_list_lower,threshold_score=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "exact : {}\n",
      "\n",
      "Approx matching\n",
      "рднреЛрдВрдХрдирд╛\n",
      "100\n",
      "slur,token : рднреЛрдВрдХрдирд╛ рднреМрдВрдХрдирд╛\n",
      "approx : {'рднреМрдВрдХрдирд╛': 'рднреЛрдВрдХрдирд╛'}\n",
      "combined : {'рднреМрдВрдХрдирд╛': 'рднреЛрдВрдХрдирд╛'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'рднреМрдВрдХрдирд╛': 'рднреЛрдВрдХрдирд╛'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slur_replacement_slurs_v1(tweet,slurs_list_lower,threshold_score=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
